1. Autoencoders learn to copy their inputs to their outputs.


2. multi-layer perceptron 모델은 input node 와 ouput node의 수가 동일하며 하나의 hidden layer를 갖는데, 이 hidden layer의 node 수는 input node보다 `1개`적다. 어떠한 activation fuction 도 설정하지 않은 상황에서 이 모델의 cost function은 MSE로 설정하여 학습을 진행한다고 가장하자. 그리고 input data는 3차원 데이터이다. 이 경우 이 모델은 `principal component analysis 처럼 한 차원 낮은 `2차원`으로 차원을 `축소`하는 역할 을 할 수 있다.

3. 사진의 캡션을 만드는 Recurrent neural network(RNN) 을 모델링 하고자 한다. input은 2D 사진 한 장씩 구성된 데이터 셋이고, output은 여러 단어로 이루어진 문장이 되는 것을 목표로 한다 . 이때 적적한 RNN type 은 `Vector-to-Sequence`이다.

4. RNN을 학습하기 위한 알고리즘으로 Backpropagatin throught time(BPTT)를 사용하려 한다. BPTT를 아래와 같이 4개의 절차로 나누었다고 가정한다면, BPTT의 학습 순서를 왼쪽부터 차례대로 나열하면 다음 과 같다.
    - unrolled net에서 forward pass를 수행하는 것
    - cost function을 사용하여 output sequence를 평가하는 것
    - gradients를 backward로 전파(propagate)하는 것
    - model parameters를 갱신(update)하는 것

5. review를 바탕으로 긍정인지 부정인지에 대한 의미를 파악하는 RNN기반의 모델을 만들려 한다. 
    - (1) 먼저 embedding을 한다.
    - (2) convolutional layer를 거친다.
    - (3) max pooling layer를 거친다.
    - (4) long short-term memory layer를 거친다.
    - (5) 최종적으로 1개의 노드를 갖는 dense layer(또는 fully connected layer)에서 모델의 출력값이 나오게 만든다.
<img src="https://user-images.githubusercontent.com/45276804/101640499-ec988280-3a73-11eb-81d6-280bfcb6a572.png">

    - 이 레이어데 대한 activation function 설정에 관하여, 가장 정확한 의미를 파악할 수 있도록 하는 방법은 logistic sigmoid function을 사용하는 것이다.
    - `ReLU나 leak ReLU는  exploding gradient problem이 있을 수 있으며, softmax 는 binary clasification에 적합하지 않다.`

6. 2014년에 발표된 GoogleNet의 Inception 모듈에서 NxN은 커널사이즈가  N by N을 의미하며, `+1 pooling layer`를 의미한다. 이 때, 이 inception 모듈의 input의 해상도는 7x7인 영상이 들어가고 (a)는 fiter개수가 64, (b) 128개, (c)는 32개 ,(d) 는 32 개인 2D convolution layer라고 할 때, `Depth concat`에서 `concatenatino`을 하고 난 이후에 filter의 총 개수는 `256`개이다.
    <img src = "https://user-images.githubusercontent.com/45276804/101640688-29fd1000-3a74-11eb-9bdf-fab6007afe25.png">
    - 해상도 상관없이 필터의 개수만 묻는 문제이다. 따라서, (a)부터 (d)까지의 모든 필터의 총합은 256이다.

7. Variational autoencoders(VAEs)에 대한 설명은 다음 과 같다.
    - VAEs are probabilistic autoencoders, so their outputs are partly determined by chance, even after training.
    - VAEs are generatvie autoencoders which can generate new instances that look like they were sampled from the trainig set.
    - The encoder of VAE produces a mean u and a standard devitaion b.
    - VAEs are of overcomplete autoencdoer.

8. MLP, CNN 들은 모두 feedforwad 형태의 NN
   - RNN은 backward connection 이 있다.

9. At time t1 recurrent neurons receives two inputs. Chose two inputs among below options.
    - x(t)
    - y(t-1)

10. 아래의 그림처럼, 시간 t에 따라 x(t)의 값이 존재하는 time series 데이터가 주어졌다. 그리고 50번째 time step 인 파란 x표시를 예측하기 위해 신경망을 모델링 하려 한다. 미래 예측을 위해 사용 가능한 신경망 종류를 모두 고르시오.
    - multi-layer perceptron
    - recurrent neural network with one hidden layer
    - perceptron
    - deep recurrent neural network


11. DNN을 구현 할 때 주요 파라미터 및 하이퍼파라미터 설정에 관한 설명으로
    - 오차함수(목적함수)중에는 MSE와 cross-entropy등의 함수가 있는데, MSE 같은 경우 entropy 계열과는 다르게 수렴하기 까지 속도가 많이 걸릴 수 있으며 regression에 유리한 경우가 있다.
    - 보통  DNN 학습을 진행할 때, 얼마나 학습을 반복할 지 정하기 위해 `epoch`이라는 파라미터가 있다. 이 때 1 epoch의 의미는 `batch size가 정해진 경우라 하더라도 모든 학습 샘플(training sample)에 대해 학습을 실행하는 것`을 의미한다.
    - 다중분류 문제에서 출력값이 0과 1로 이루어지게 변환하고 싶은 경우, `one-hotencoding`기법을 사용 할 수 있다.

12. Back-propagation(BP) trainig algorithm
    - MLP에서 사용한다면, step function은 고ㅛ체되어야 한다. 왜냐하면 step fuction은 gradient 계산이 불가능한 곳이 존재하기 때문
    - BP는 MLP의 모든 노드간에 연결된 모든 weight와 bias값들을 조정 할 수 있다.
    - BP를 학습하는 방식 중에 forward pass에서 계산한 예측값(출력값)들을 저장하면 그 값을 수정없이 그대로 backward pass에서의 gradient를 계산할 때에도 사용할 수 있다.
    - DNN에서 BP는 actication fuction들 중에 logistic sigmod, hyperbolic tangent, ReLU와도 학습을 진행할 수 있으며, 이 중 미분값이 가장 작은 함수는 sigmoid 이다.

13. 비지도학습의 하나로 K-Means 알고리즘은 clustering 작업을 수행하는 효율적이고 매우 빠른 기계학습 알고리즘이다. 그러나 몇 가지 단점이 존재하는데, 이러한 단점을 해결하기 위해
    - Suboptimal solutions due to unlucky centroid initializations.
    - K-Means needs to use the full dataset at each iteration during trainig, so its time and memory complexity become critical when clustsers go huge.
    - K-Means does noe behave very well when the clusters have varying sizes, different densities, or nonospherical shapes.
    - Since K-Means is unsupervised learning algorithm, the dataset has no label.

14. Backpropagation algorithm의 핵심을 3단계로 요약하면 `Forward pass, Backward(reverse)pass, Gradient Descent의 3가지로 설명할 수 있다.
    - The algorithm then measures how much of these error contributions came from each connection in the layer below, working backward until the algorithm reaches the input layer. This reverse pass efficiently measures the error gradient across all the connection eights in the network by propagating the error gradient backward through the network.

15. Kernel Trick이 필요한 경우와 한계
    - 내적이 가능해야 kernel trick을 쓸 수 있다.
    - 원 특징공간이 매우 고차원인 상태에서 더 고차원으로 변환하는 것이 불가능한 경우, Kernel Trick 을 이용하여 극복할 수 있다.
    - 본래 선형 분리가 불가능했던 2차원 공간을 선형 분리가 가능한 공간으로 변환한 후, 해당 변환 공간에 매핑(mapping)한 샘플들을 ML알고리즘에 학습데이터로 적용하여 학습시키는 방법은 Kernal Trick을 적용한 것이 아니다.
    - PCA에 kernel trick을 적용하고자 할 경우, 아래의 전처리/가정이 있을 때 보다 효율적으로 kernel trick을 계산하고 적용할 수 있다.
    - 전처리 : 원 데이터를 원점 중심으로 옮기는 전처리를 진행 먼저 진행함.
    - 가정 : 데이터들이 원점을 중심으로 분포되었다고 추정하는 변환 공간이 있다고 가정한 이후에  kernel trick을 적용

16. Why would you want to use 1D convolutional layer in an RNN?
    - To reuce their temporal resolution (downsampling)  and thereby help the RNN layers detect long - term patterns.

17. What can we use to solve gradients problem in RNN?
    - Saturating activation functin (e.g., sigmoid and hyperbolic tangent)
    - Layer normalization

18. What are the main difficultie swhen training RNNs?
    - Unstable gradients
    - very limited short-term memory

19. 4층 퍼셉트론 표현 o=f4(f3(f2(f1(x))))
    - 은닉층 1~3, 출력층 4

20. 활성화함수(activation function)을 통해 분류문제를 결정할 때, 공간분할을 가장 딱딱하게 하는 함수는 `step function`
    - `logistic sigmoid function`, `hyperbolic tangent function`, `rectified linear unit function` 부드러운 의사결정을 이룰 수 있게 해준다.

